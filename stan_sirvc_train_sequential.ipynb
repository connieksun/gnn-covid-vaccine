{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\r\n",
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "#from owid_downloader import GenerateTrainingData\r\n",
    "#from utils import date_today, gravity_law_commute_dist\r\n",
    "\r\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '16'\r\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '8'\r\n",
    "\r\n",
    "import pickle\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import dgl\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from model import STAN\r\n",
    "\r\n",
    "import sklearn\r\n",
    "from sklearn.metrics import mean_absolute_error\r\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#GenerateTrainingData().download_jhu_data('2020-08-01', '2020-12-01')\r\n",
    "start_date = '2021-01-01'\r\n",
    "end_date = '2021-05-31'"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data processing\r\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv', usecols=[\"location\", \"date\", \"total_cases\", \"new_cases_smoothed\", \"total_deaths\",\r\n",
    "                    \"new_deaths\", \"total_vaccinations\", \"people_fully_vaccinated\", \"new_vaccinations\", \"population\"])\r\n",
    "raw_data['date'] = pd.to_datetime(raw_data['date'])\r\n",
    "mask = (raw_data['date'] >= start_date) & (raw_data['date'] <= end_date) # & (raw_data['location'].isin(countries))\r\n",
    "raw_data = raw_data.loc[mask]\r\n",
    "#print(raw_data[raw_data['location'] == 'United States']['total_cases'].values[0])\r\n",
    "countries = []\r\n",
    "loc_list = list(raw_data['location'].unique())\r\n",
    "# only include countries that have more than 1000 total cases on start date and at least 1 death\r\n",
    "for loc in loc_list:\r\n",
    "    if raw_data[raw_data['location'] == loc][\"total_cases\"].values[0] > 1000 and \\\r\n",
    "        raw_data[raw_data['location'] == loc][\"total_deaths\"].values[0] > 0:\r\n",
    "        countries.append(loc)\r\n",
    "# hard-coded; these are problematic locations (non-countries) that need to be removed\r\n",
    "countries.remove(\"European Union\")\r\n",
    "countries.remove(\"Europe\")\r\n",
    "countries.remove(\"Africa\")\r\n",
    "countries.remove(\"Asia\")\r\n",
    "countries.remove(\"North America\")\r\n",
    "countries.remove(\"Oceania\")\r\n",
    "countries.remove(\"South America\")\r\n",
    "countries.remove(\"World\")\r\n",
    "countries.remove(\"Tajikistan\")\r\n",
    "mask = raw_data['location'].isin(countries)\r\n",
    "raw_data = raw_data.loc[mask]\r\n",
    "print(len(raw_data['location'].unique()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate Graph\r\n",
    "# add flight neighbors\r\n",
    "# for now, add a connection if there is any flight between the two countries between start and end date\r\n",
    "loc_list = list(raw_data['location'].unique())\r\n",
    "flight_counts = pd.read_csv('processed_flights/flight_counts_2021_all_to_05.csv')\r\n",
    "adj_map = {}\r\n",
    "for each_loc in loc_list:\r\n",
    "    df = flight_counts.loc[flight_counts[\"origin_country\"] == each_loc]\r\n",
    "    adj_map[each_loc] = set(df[\"destination_country\"].unique())\r\n",
    "flight_counts['day'] = pd.to_datetime(flight_counts['day'])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add land neighbors\r\n",
    "import csv\r\n",
    "neighbor_reader = csv.reader(open('neighbors.csv', 'r'))\r\n",
    "neighbors = {}\r\n",
    "for row in neighbor_reader:\r\n",
    "   neighbors[row[0]] = row[1].split(',')\r\n",
    "for each_loc,connected in adj_map.items():\r\n",
    "    for neighbor in neighbors[each_loc]:\r\n",
    "        if neighbor in loc_list:\r\n",
    "            connected.add(neighbor)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create graph\r\n",
    "rows = []\r\n",
    "cols = []\r\n",
    "for each_loc in adj_map:\r\n",
    "    for each_loc2 in adj_map[each_loc]:\r\n",
    "        if each_loc in loc_list and each_loc2 in loc_list:\r\n",
    "            rows.append(loc_list.index(each_loc))\r\n",
    "            cols.append(loc_list.index(each_loc2))\r\n",
    "#print(rows)\r\n",
    "#print(cols)\r\n",
    "g = dgl.graph((rows, cols))\r\n",
    "print(g.number_of_nodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import networkx as nx\r\n",
    "# nx_g = g.to_networkx()\r\n",
    "# pos = nx.kamada_kawai_layout(nx_g)\r\n",
    "# plt.figure(1,figsize=(8,8)) \r\n",
    "# nx.draw(nx_g, pos, with_labels=True, node_color=[[.7, .7, .7]])\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Preprocess features\r\n",
    "\r\n",
    "#active_cases = []\r\n",
    "confirmed_cases = []\r\n",
    "new_cases = []\r\n",
    "new_vaccinations = []\r\n",
    "fully_vaccinated = []\r\n",
    "death_cases = []\r\n",
    "static_feat = []\r\n",
    "\r\n",
    "for i, each_loc in enumerate(loc_list):\r\n",
    "    confirmed_cases.append(raw_data[raw_data['location'] == each_loc]['total_cases'])\r\n",
    "    new_cases.append(raw_data[raw_data['location'] == each_loc]['new_cases_smoothed'])\r\n",
    "    new_vaccinations.append(raw_data[raw_data['location'] == each_loc]['new_vaccinations'])\r\n",
    "    fully_vaccinated.append(raw_data[raw_data['location'] == each_loc]['people_fully_vaccinated'])\r\n",
    "    death_cases.append(raw_data[raw_data['location'] == each_loc]['total_deaths'])\r\n",
    "    static_feat.append(np.array(raw_data[raw_data['location'] == each_loc][['population']]))\r\n",
    "confirmed_cases = np.nan_to_num(np.array(confirmed_cases))\r\n",
    "death_cases = np.nan_to_num(np.array(death_cases)[:, 14:])\r\n",
    "new_cases = np.nan_to_num(np.array(new_cases)[:, 14:])\r\n",
    "new_vaccinations = np.nan_to_num(np.array(new_vaccinations)[:, 14:])\r\n",
    "fully_vaccinated = np.nan_to_num(np.array(fully_vaccinated))\r\n",
    "static_feat = np.nan_to_num(np.array(static_feat)[:, 0, :])\r\n",
    "\r\n",
    "import copy\r\n",
    "# active = confirmed(today) - confirmed(14 days ago)\r\n",
    "cases_copy = copy.deepcopy(confirmed_cases)\r\n",
    "active = []\r\n",
    "for loc in confirmed_cases:\r\n",
    "    active_loc = []\r\n",
    "    for i in range(14, len(loc)):\r\n",
    "        active_loc.append(loc[i] - loc[i-14])\r\n",
    "    active.append(active_loc)\r\n",
    "active_cases = np.array(active)\r\n",
    "\r\n",
    "confirmed_cases = confirmed_cases[:, 14:]\r\n",
    "fully_vaccinated = fully_vaccinated[:, 14:]\r\n",
    "\r\n",
    "recovered_cases = confirmed_cases - active_cases - death_cases + 0.94*fully_vaccinated\r\n",
    "susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\r\n",
    "\r\n",
    "# Batch_feat: new_cases(dI), dR, dS\r\n",
    "#dI = np.array(new_cases)\r\n",
    "dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases)), axis=-1)\r\n",
    "dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases)), axis=-1)\r\n",
    "dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases)), axis=-1)\r\n",
    "# number of new fully vaccinated each day\r\n",
    "Vt = np.concatenate((np.zeros((fully_vaccinated.shape[0],1), dtype=np.float32), np.diff(fully_vaccinated)), axis=-1)\r\n",
    "print(\"done\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Build normalizer\r\n",
    "normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}, 'Vt':{}}\r\n",
    "\r\n",
    "for i, each_loc in enumerate(loc_list):\r\n",
    "    normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\r\n",
    "    normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\r\n",
    "    normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\r\n",
    "    normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\r\n",
    "    normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\r\n",
    "    normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))\r\n",
    "    normalizer['Vt'][each_loc] = (np.mean(Vt[i]), np.std(Vt[i]))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_data(data, sum_I, sum_R, Vt, edges_df, start, history_window=8, pred_window=14, slide_step=7):\r\n",
    "    # Data shape n_loc, timestep, n_feat\r\n",
    "    # Reshape to n_loc, t, history_window*n_feat\r\n",
    "    n_loc = data.shape[0]\r\n",
    "    timestep = data.shape[1]\r\n",
    "    n_feat = data.shape[2]\r\n",
    "    \r\n",
    "    x = []\r\n",
    "    y_I = []\r\n",
    "    y_R = []\r\n",
    "    last_I = []\r\n",
    "    last_R = []\r\n",
    "    concat_I = []\r\n",
    "    concat_R = []\r\n",
    "    concat_Vt = []\r\n",
    "    edges = []\r\n",
    "    for i in range(0, timestep, slide_step):\r\n",
    "        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\r\n",
    "            break\r\n",
    "        x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\r\n",
    "        \r\n",
    "        concat_I.append(data[:, i+history_window-1, 0])\r\n",
    "        concat_R.append(data[:, i+history_window-1, 1])\r\n",
    "        last_I.append(sum_I[:, i+history_window-1])\r\n",
    "        last_R.append(sum_R[:, i+history_window-1])\r\n",
    "\r\n",
    "        y_I.append(data[:, i+history_window:i+history_window+pred_window, 0])\r\n",
    "        y_R.append(data[:, i+history_window:i+history_window+pred_window, 1])\r\n",
    "\r\n",
    "        concat_Vt.append(Vt[:, i+history_window:i+history_window+pred_window])\r\n",
    "        \r\n",
    "        e_matrix = np.zeros((n_loc, n_loc)) # edge weight matrix for every time period \r\n",
    "        df = edges_df.groupby([\"origin_country\"])\r\n",
    "        for loc in range(n_loc):\r\n",
    "            try:\r\n",
    "                src_df = df.get_group(loc_list[loc])\r\n",
    "                src_df = src_df.groupby([\"destination_country\"])\r\n",
    "                for loc2 in range(n_loc):\r\n",
    "                    try:\r\n",
    "                        dst_df = src_df.get_group(loc_list[loc2])\r\n",
    "                        dst_df = dst_df.loc[(dst_df['day'] >= (pd.to_datetime(start) + pd.DateOffset(days=i)))]\r\n",
    "                        dst_df = dst_df.loc[(dst_df['day'] < (pd.to_datetime(start) + pd.DateOffset(days=i+history_window-1)))]\r\n",
    "                        e_matrix[loc, loc2] = dst_df['flight_count'].sum()\r\n",
    "                    except:\r\n",
    "                        continue\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        # normalize matrix (doubly stochastic, see https://arxiv.org/pdf/1809.02709.pdf)\r\n",
    "        # step 1: row normalize\r\n",
    "        norm = np.sum(e_matrix, axis=1, keepdims=True)\r\n",
    "        norm[norm==0] = 1e-10\r\n",
    "        norm = 1.0 / norm\r\n",
    "        P = e_matrix * norm\r\n",
    "\r\n",
    "        # step 2: P @ P^T / column_norm\r\n",
    "        norm = np.sum(P, axis=0, keepdims=True)\r\n",
    "        norm[norm==0] = 1e-10\r\n",
    "        norm = 1.0 / norm\r\n",
    "\r\n",
    "        PT = np.transpose(P, (1, 0))\r\n",
    "        P = np.multiply(P, norm)\r\n",
    "        T = np.matmul(P, PT)\r\n",
    "        edges.append(T) # n_rows = # countries, n_cols = # countries\r\n",
    "        \r\n",
    "    \r\n",
    "    x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\r\n",
    "    last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\r\n",
    "    last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\r\n",
    "    concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\r\n",
    "    concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\r\n",
    "    y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\r\n",
    "    y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\r\n",
    "    concat_Vt = np.array(concat_Vt, dtype=np.float32).transpose((1, 0, 2))\r\n",
    "    return x, last_I, last_R, concat_I, concat_R, y_I, y_R, concat_Vt, edges"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def squish_edges(E):\r\n",
    "    # use src and dst (rows and cols) to make E matrix\r\n",
    "    edges = []\r\n",
    "    for M in E:\r\n",
    "        edges_flat = []\r\n",
    "        for i in range(len(rows)):\r\n",
    "            edges_flat.append(M[rows[i]][cols[i]])\r\n",
    "        edges.append(edges_flat)\r\n",
    "    return np.array(edges)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_window = 29\r\n",
    "test_window = 29\r\n",
    "\r\n",
    "history_window=14 # days of information\r\n",
    "pred_window=14 # predicts future # of days\r\n",
    "slide_step=3 # increment\r\n",
    "\r\n",
    "dynamic_feat = np.concatenate((np.expand_dims(dI, axis=-1), np.expand_dims(dR, axis=-1), np.expand_dims(dS, axis=-1)), axis=-1)\r\n",
    "    \r\n",
    "#Normalize\r\n",
    "for i, each_loc in enumerate(loc_list):\r\n",
    "    dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\r\n",
    "    dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\r\n",
    "    dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\r\n",
    "    # vaccinations don't need to be normalized\r\n",
    "    #mean_vax = normalizer['Vt'][each_loc][0]\r\n",
    "    #if mean_vax != 0:\r\n",
    "    #   Vt[i] = (Vt[i] - mean_vax) / normalizer['Vt'][each_loc][1]\r\n",
    "dI_mean = []\r\n",
    "dI_std = []\r\n",
    "dR_mean = []\r\n",
    "dR_std = []\r\n",
    "\r\n",
    "for i, each_loc in enumerate(loc_list):\r\n",
    "    dI_mean.append(normalizer['dI'][each_loc][0])\r\n",
    "    dR_mean.append(normalizer['dR'][each_loc][0])\r\n",
    "    dI_std.append(normalizer['dI'][each_loc][1])\r\n",
    "    dR_std.append(normalizer['dR'][each_loc][1])\r\n",
    "\r\n",
    "dI_mean = np.array(dI_mean)\r\n",
    "dI_std = np.array(dI_std)\r\n",
    "dR_mean = np.array(dR_mean)\r\n",
    "dR_std = np.array(dR_std)\r\n",
    "\r\n",
    "#Split train-test\r\n",
    "train_feat = dynamic_feat[:, :-valid_window-test_window, :]\r\n",
    "val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\r\n",
    "test_feat = dynamic_feat[:, -test_window:, :]\r\n",
    "\r\n",
    "valid_start_date = pd.to_datetime(end_date) + pd.DateOffset(days=-valid_window) + pd.DateOffset(days=-test_window)\r\n",
    "test_start_date = pd.to_datetime(end_date) + pd.DateOffset(days=-test_window)\r\n",
    "\r\n",
    "train_edges = flight_counts[(flight_counts[\"day\"] >= start_date) & (flight_counts[\"day\"] < valid_start_date)]\r\n",
    "val_edges = flight_counts[(flight_counts[\"day\"] >= valid_start_date) & (flight_counts[\"day\"] < test_start_date)]\r\n",
    "test_edges = flight_counts[(flight_counts[\"day\"] >= test_start_date) & (flight_counts[\"day\"] < end_date)]\r\n",
    "\r\n",
    "train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR, train_Vt, train_edges = prepare_data(train_feat, active_cases[:, :-valid_window-test_window], recovered_cases[:, :-valid_window-test_window], Vt[:, :-valid_window-test_window], train_edges, start_date, history_window, pred_window, slide_step)\r\n",
    "\r\n",
    "val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR, val_Vt, val_edges = prepare_data(val_feat, active_cases[:, -valid_window-test_window:-test_window], recovered_cases[:, -valid_window-test_window:-test_window], Vt[:, -valid_window-test_window:-test_window], val_edges, valid_start_date, history_window, pred_window, slide_step)\r\n",
    "\r\n",
    "test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR, test_Vt, test_edges = prepare_data(test_feat, active_cases[:, -test_window:], recovered_cases[:, -test_window:], Vt[:, -test_window:], test_edges, test_start_date, history_window, pred_window, slide_step)\r\n",
    "\r\n",
    "train_edges = squish_edges(train_edges)\r\n",
    "val_edges = squish_edges(val_edges)\r\n",
    "test_edges = squish_edges(test_edges)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_edges.shape) # one edge array (len = # edges) for each timestep\r\n",
    "print(train_x.shape) # one array of features for each timestep for each location\r\n",
    "print(val_edges.shape)\r\n",
    "print(val_x.shape)\r\n",
    "print(test_edges.shape)\r\n",
    "print(test_x.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Build STAN model\r\n",
    "\r\n",
    "in_dim = 3*history_window\r\n",
    "hidden_dim1 = 32\r\n",
    "hidden_dim2 = 32\r\n",
    "gru_dim = 32\r\n",
    "num_heads = 1\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "g = g.to(device)\r\n",
    "model = STAN(g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device).to(device)\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\r\n",
    "criterion = nn.MSELoss()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x = torch.tensor(train_x).to(device)\r\n",
    "train_I = torch.tensor(train_I).to(device)\r\n",
    "train_R = torch.tensor(train_R).to(device)\r\n",
    "train_cI = torch.tensor(train_cI).to(device)\r\n",
    "train_cR = torch.tensor(train_cR).to(device)\r\n",
    "train_yI = torch.tensor(train_yI).to(device)\r\n",
    "train_yR = torch.tensor(train_yR).to(device)\r\n",
    "train_Vt = torch.tensor(train_Vt).to(device)\r\n",
    "train_edges = torch.tensor(train_edges).to(device)\r\n",
    "\r\n",
    "val_x = torch.tensor(val_x).to(device)\r\n",
    "val_I = torch.tensor(val_I).to(device)\r\n",
    "val_R = torch.tensor(val_R).to(device)\r\n",
    "val_cI = torch.tensor(val_cI).to(device)\r\n",
    "val_cR = torch.tensor(val_cR).to(device)\r\n",
    "val_yI = torch.tensor(val_yI).to(device)\r\n",
    "val_yR = torch.tensor(val_yR).to(device)\r\n",
    "val_Vt = torch.tensor(val_Vt).to(device)\r\n",
    "val_edges = torch.tensor(val_edges).to(device)\r\n",
    "\r\n",
    "test_x = torch.tensor(test_x).to(device)\r\n",
    "test_I = torch.tensor(test_I).to(device)\r\n",
    "test_R = torch.tensor(test_R).to(device)\r\n",
    "test_cI = torch.tensor(test_cI).to(device)\r\n",
    "test_cR = torch.tensor(test_cR).to(device)\r\n",
    "test_yI = torch.tensor(test_yI).to(device)\r\n",
    "test_yR = torch.tensor(test_yR).to(device)\r\n",
    "test_Vt = torch.tensor(test_Vt).to(device)\r\n",
    "test_edges = torch.tensor(test_edges).to(device)\r\n",
    "\r\n",
    "dI_mean = torch.tensor(dI_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\r\n",
    "dI_std = torch.tensor(dI_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\r\n",
    "dR_mean = torch.tensor(dR_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\r\n",
    "dR_std = torch.tensor(dR_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\r\n",
    "\r\n",
    "N = torch.tensor(static_feat[:, 0], dtype=torch.float32).to(device).unsqueeze(-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_adaptive_loss(pred_I, pred_I_sir, pred_R, pred_R_sir, true_I, true_R, pred_window=14):\r\n",
    "    total_loss = 0\r\n",
    "    for timestep in range(len(pred_I)):\r\n",
    "        for day in range(1, pred_window+1):\r\n",
    "            sir_weight = day/(pred_window+1)\r\n",
    "            pred_weight = 1-sir_weight\r\n",
    "            sir_loss = sir_weight*((pred_I_sir[timestep][day-1] - true_I[timestep][day-1])**2) + sir_weight*((pred_R_sir[timestep][day-1] - true_R[timestep][day-1])**2)\r\n",
    "            pred_loss = pred_weight*((pred_I[timestep][day-1] - true_I[timestep][day-1])**2) + pred_weight*((pred_R[timestep][day-1] - true_R[timestep][day-1])**2)\r\n",
    "            total_loss += sir_loss + pred_loss\r\n",
    "    return total_loss / pred_window\r\n",
    "\r\n",
    "def val_adaptive_loss(pred_I, pred_I_sir, true_I, pred_window=14):\r\n",
    "    total_loss = 0\r\n",
    "    for day in range(1, pred_window+1):\r\n",
    "        sir_weight = day/(pred_window+1)\r\n",
    "        pred_weight = 1-sir_weight\r\n",
    "        sir_loss = sir_weight*((pred_I_sir[day-1] - true_I[day-1])**2)\r\n",
    "        pred_loss = pred_weight*((pred_I[day-1] - true_I[day-1])**2)\r\n",
    "        total_loss += sir_loss + pred_loss\r\n",
    "    return total_loss / pred_window"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Train STAN\r\n",
    "all_loss = []\r\n",
    "file_name = './save/stan'\r\n",
    "min_loss = 1e20\r\n",
    "\r\n",
    "train_locs = [\"United States\", \"United Kingdom\"]\r\n",
    "\r\n",
    "for epoch in range(50):\r\n",
    "    global_train_loss = 0\r\n",
    "    global_val_loss = 0\r\n",
    "    for loc in train_locs:\r\n",
    "        cur_loc = loc_list.index(loc)\r\n",
    "        model.train()\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        active_pred, recovered_pred, phy_active, phy_recover, _ = model(train_x, train_cI[cur_loc], train_cR[cur_loc], N[cur_loc], train_I[cur_loc], train_R[cur_loc], V=train_Vt[cur_loc], e_weights=train_edges)\r\n",
    "        phy_active = (phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\r\n",
    "        phy_recover = (phy_recover - dR_mean[cur_loc]) / dR_std[cur_loc]\r\n",
    "        # SIR loss = (day) / (pred_window + 1); dynamics loss = 1 - SIR loss \r\n",
    "        #loss = criterion(active_pred.squeeze(), train_yI[cur_loc].squeeze())+criterion(recovered_pred.squeeze(), train_yR[cur_loc].squeeze()) \\\r\n",
    "        #    + 0.1*criterion(phy_active.squeeze(), train_yI[cur_loc].squeeze())+0.1*criterion(phy_recover.squeeze(), train_yR[cur_loc].squeeze())\r\n",
    "        loss = train_adaptive_loss(active_pred.squeeze(), phy_active.squeeze(), recovered_pred.squeeze(), phy_recover.squeeze(), train_yI[cur_loc].squeeze(), train_yR[cur_loc].squeeze(), pred_window)\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        all_loss.append(loss.item())\r\n",
    "        \r\n",
    "        model.eval()\r\n",
    "        _, _, _, _, prev_h = model(train_x, train_cI[cur_loc], train_cR[cur_loc], N[cur_loc], train_I[cur_loc], train_R[cur_loc], V=train_Vt[cur_loc], e_weights=train_edges)\r\n",
    "        val_active_pred, val_recovered_pred, val_phy_active, val_phy_recover, _ = model(val_x, val_cI[cur_loc], val_cR[cur_loc], N[cur_loc], val_I[cur_loc], val_R[cur_loc], prev_h, V=val_Vt[cur_loc], e_weights=val_edges)\r\n",
    "        \r\n",
    "        val_phy_active = (val_phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\r\n",
    "        # SIR loss = (day) / (pred_window + 1); dynamics loss = 1 - SIR loss \r\n",
    "        # change loss here \r\n",
    "        #val_loss = criterion(val_active_pred.squeeze(), val_yI[cur_loc].squeeze()) + 0.1*criterion(val_phy_active.squeeze(), val_yI[cur_loc].squeeze())\r\n",
    "        val_loss = val_adaptive_loss(val_active_pred.squeeze(), val_phy_active.squeeze(), val_yI[cur_loc].squeeze(), pred_window)\r\n",
    "        global_train_loss += loss\r\n",
    "        global_val_loss += val_loss\r\n",
    "    if (global_val_loss + global_train_loss) / 2 < min_loss:   \r\n",
    "        state = {\r\n",
    "            'state': model.state_dict(),\r\n",
    "            'optimizer': optimizer.state_dict(),\r\n",
    "        }\r\n",
    "        torch.save(state, file_name)\r\n",
    "        min_loss = (global_val_loss + global_train_loss) / 2\r\n",
    "        print('-----Save best model-----')\r\n",
    "    print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, global_train_loss.item(), global_val_loss.item()))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Pred with STAN\r\n",
    "file_name = './save/stan'\r\n",
    "checkpoint = torch.load(file_name)\r\n",
    "model.load_state_dict(checkpoint['state'])\r\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\r\n",
    "model.eval()\r\n",
    "\r\n",
    "prev_x = torch.cat((train_x, val_x), dim=1)\r\n",
    "prev_I = torch.cat((train_I, val_I), dim=1)\r\n",
    "prev_R = torch.cat((train_R, val_R), dim=1)\r\n",
    "prev_cI = torch.cat((train_cI, val_cI), dim=1)\r\n",
    "prev_cR = torch.cat((train_cR, val_cR), dim=1)\r\n",
    "prev_Vt = torch.cat((train_Vt, val_Vt), dim=1)\r\n",
    "prev_edges = torch.cat((train_edges, val_edges), dim=0)\r\n",
    "\r\n",
    "train_locs.reverse()\r\n",
    "\r\n",
    "for loc_name in train_locs:\r\n",
    "    cur_loc = loc_list.index(loc_name)\r\n",
    "    prev_active_pred, _, prev_phy_active_pred, _, h = model(prev_x, prev_cI[cur_loc], prev_cR[cur_loc], N[cur_loc], prev_I[cur_loc], prev_R[cur_loc], V=prev_Vt[cur_loc], e_weights=prev_edges)\r\n",
    "\r\n",
    "    test_pred_active, test_pred_recovered, test_pred_phy_active, test_pred_phy_recover, _ = model(test_x, test_cI[cur_loc], test_cR[cur_loc], N[cur_loc], test_I[cur_loc], test_R[cur_loc], h, V=test_Vt[cur_loc], e_weights=test_edges)\r\n",
    "\r\n",
    "    print('Estimated beta in SIR model is %.2f'%model.alpha_scaled)\r\n",
    "    print('Estimated gamma in SIR model is %.2f'%model.beta_scaled)\r\n",
    "    print('Estimated theta in SIR model is %.2f'%model.theta_scaled)\r\n",
    "\r\n",
    "    #Cumulate predicted dI\r\n",
    "    pred_I = []\r\n",
    "    sir_I = []\r\n",
    "\r\n",
    "    for i in range(test_pred_active.size(1)):\r\n",
    "        # below is regular prediction\r\n",
    "        cur_pred = (test_pred_active[0, i, :].detach().cpu().numpy() * dI_std[cur_loc].reshape(1, 1).detach().cpu().numpy()) + dI_mean[cur_loc].reshape(1, 1).detach().cpu().numpy()\r\n",
    "        # below is SIR model prediction\r\n",
    "        sir_pred = test_pred_phy_active[0, i, :].detach().cpu().numpy()\r\n",
    "        # below is average of the two predictions\r\n",
    "        #cur_pred = (cur_pred + test_pred_phy_active[0, i, :].detach().cpu().numpy()) / 2\r\n",
    "        cur_pred = np.cumsum(cur_pred)\r\n",
    "        cur_pred = cur_pred + test_I[cur_loc, i].detach().cpu().item()\r\n",
    "        pred_I.append(cur_pred)\r\n",
    "\r\n",
    "        sir_pred = np.cumsum(sir_pred)\r\n",
    "        sir_pred = sir_pred + test_I[cur_loc, i].detach().cpu().item()\r\n",
    "        sir_I.append(sir_pred)\r\n",
    "    pred_I = np.array(pred_I)\r\n",
    "    sir_I = np.array(sir_I)\r\n",
    "\r\n",
    "    #I_true = get_real_y(active_cases[:], history_window, pred_window, slide_step)\r\n",
    "    I_true = active_cases[cur_loc][-test_window:]\r\n",
    "    I_true = I_true[history_window:history_window+pred_window]\r\n",
    "\r\n",
    "\r\n",
    "    import matplotlib.dates as mdates\r\n",
    "    dates = [test_start_date + pd.DateOffset(days=i+history_window) for i in range(pred_window)]\r\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%Y'))\r\n",
    "    #plt.plot(I_true[cur_loc, -1, :],c='r', label='Ground truth')\r\n",
    "    stan_res = pickle.load(open(f\"stan_results/{loc_name}_stan_results_new\", \"rb\"))\r\n",
    "    stan_sirv_res = pickle.load(open(f\"stan_results/{loc_name}_stan_results_sirv\", \"rb\"))\r\n",
    "    plt.plot(dates, I_true, c='r', label='Ground truth')\r\n",
    "    plt.plot(dates, stan_res[-1, :], c='g', label=\"STAN (SIRV)\")\r\n",
    "    #plt.plot(dates, sir_I[-1, :], c='purple', label=\"SIRVIC\")\r\n",
    "    plt.plot(dates, pred_I[-1, :],c='b', label='Proposed')\r\n",
    "    plt.gcf().autofmt_xdate()\r\n",
    "    plt.legend()\r\n",
    "    plt.ylabel(\"Number of Active Cases\")\r\n",
    "    plt.title(loc_name)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    prop_mse = ((pred_I[0] - I_true)**2).mean()\r\n",
    "    sirvic_mse = ((sir_I[0] - I_true)**2).mean()\r\n",
    "    stan_mse = ((stan_res[0] - I_true)**2).mean()\r\n",
    "    print(\"MSE of Models\")\r\n",
    "    print(f\"Proposed: {prop_mse}\")\r\n",
    "    print(f\"STAN: {stan_mse}\")\r\n",
    "    print(f\"SIRVIC: {sirvic_mse}\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# prop_mse = ((pred_I[0] - I_true)**2).mean()\r\n",
    "# sirvic_mse = ((sir_I[0] - I_true)**2).mean()\r\n",
    "# stan_mse = ((stan_res[0] - I_true)**2).mean()\r\n",
    "# print(\"MSE of Models\")\r\n",
    "# print(f\"Proposed: {prop_mse}\")\r\n",
    "# print(f\"STAN: {stan_mse}\")\r\n",
    "# print(f\"SIRVIC: {sirvic_mse}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# final graph visualization\r\n",
    "import networkx as nx\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "def plot(g, attention, ax, nodes_to_plot=None, nodes_labels=None,\r\n",
    "         edges_to_plot=None, nodes_pos=None, nodes_colors=None,\r\n",
    "         edge_colormap=plt.cm.viridis, labels=[], edge_layer_2=None):\r\n",
    "    \"\"\"\r\n",
    "    Visualize edge attentions by coloring edges on the graph.\r\n",
    "    g: nx.DiGraph\r\n",
    "        Directed networkx graph\r\n",
    "    attention: list\r\n",
    "        Attention values corresponding to the order of sorted(g.edges())\r\n",
    "    ax: matplotlib.axes._subplots.AxesSubplot\r\n",
    "        ax to be used for plot\r\n",
    "    nodes_to_plot: list\r\n",
    "        List of node ids specifying which nodes to plot. Default to\r\n",
    "        be None. If None, all nodes will be plot.\r\n",
    "    nodes_labels: list, numpy.array\r\n",
    "        nodes_labels[i] specifies the label of the ith node, which will\r\n",
    "        decide the node color on the plot. Default to be None. If None,\r\n",
    "        all nodes will have the same canonical label. The nodes_labels\r\n",
    "        should contain labels for all nodes to be plot.\r\n",
    "    edges_to_plot: list of 2-tuples (i, j)\r\n",
    "        List of edges represented as (source, destination). Default to\r\n",
    "        be None. If None, all edges will be plot.\r\n",
    "    nodes_pos: dictionary mapping int to numpy.array of size 2\r\n",
    "        Default to be None. Specifies the layout of nodes on the plot.\r\n",
    "    nodes_colors: list\r\n",
    "        Specifies node color for each node class. Its length should be\r\n",
    "        bigger than number of node classes in nodes_labels.\r\n",
    "    edge_colormap: plt.cm\r\n",
    "        Specifies the colormap to be used for coloring edges.\r\n",
    "    \"\"\"\r\n",
    "    if nodes_to_plot is None:\r\n",
    "        nodes_to_plot = sorted(g.nodes())\r\n",
    "    if edges_to_plot is None:\r\n",
    "        edges_to_plot = sorted(g.edges())\r\n",
    "    nx.draw_networkx_edges(g, nodes_pos, edgelist=edges_to_plot,\r\n",
    "                           edge_color=attention, edge_cmap=edge_colormap,\r\n",
    "                           width=2, alpha=0.5, ax=ax, edge_vmin=0,\r\n",
    "                           edge_vmax=1)\r\n",
    "    if edge_layer_2 is not None:\r\n",
    "        nx.draw_networkx_edges(g, nodes_pos, edgelist=edge_layer_2[0],\r\n",
    "                           edge_color=edge_layer_2[1], edge_cmap=edge_colormap,\r\n",
    "                           width=2, alpha=0.5, ax=ax, edge_vmin=0,\r\n",
    "                           edge_vmax=1)\r\n",
    "\r\n",
    "    if nodes_colors is None:\r\n",
    "        nodes_colors = sns.color_palette(\"deep\", max(nodes_labels) + 1)\r\n",
    "\r\n",
    "    nx.draw_networkx_nodes(g, nodes_pos, nodelist=nodes_to_plot, ax=ax, node_size=20,\r\n",
    "                           node_color=[nodes_colors[nodes_labels[v - 1]] for v in nodes_to_plot],\r\n",
    "                            alpha=0.9)\r\n",
    "    loc_labels = {}\r\n",
    "    for node in g.nodes():\r\n",
    "        if node in labels:\r\n",
    "            loc_labels[node] = loc_list[node]\r\n",
    "    nx.draw_networkx_labels(g, nodes_pos, loc_labels, font_size=16)\r\n",
    "\r\n",
    "nx_g = model.g.cpu().to_networkx(edge_attrs=['e'])\r\n",
    "#print(nx_g.nodes())\r\n",
    "attention = []\r\n",
    "mid_attention = []\r\n",
    "mid_edges = []\r\n",
    "for e in sorted(nx_g.edges()):\r\n",
    "    a = nx_g.get_edge_data(e[0], e[1])[0]['e'].item()\r\n",
    "    attention.append(a)\r\n",
    "    if a > 0.1 and a < 0.9:\r\n",
    "        mid_attention.append(a)\r\n",
    "        mid_edges.append(e)\r\n",
    "labels = [0 if i not in train_locs else 1 for i in range(len(nx_g.nodes()))]\r\n",
    "loc_i_list = [loc_list.index(loc) for loc in train_locs]\r\n",
    "pos = nx.spring_layout(nx_g, k=0.25, iterations=20)  # positions for all nodes\r\n",
    "\r\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\r\n",
    "plot(nx_g, attention, ax, nodes_pos=pos, nodes_labels=labels, nodes_colors=[\"red\" if i not in train_locs else \"blue\" for i in range(len(nx_g.nodes()))], edge_layer_2=(mid_edges, mid_attention))\r\n",
    "ax.set_axis_off()\r\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=1))\r\n",
    "sm.set_array([])\r\n",
    "plt.colorbar(sm, fraction=0.046, pad=0.01)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "cdfa9ace6613655d28fa2370fe498fbea8375f41a5fc4e643d7f8a581612fdbd"
   }
  },
  "interpreter": {
   "hash": "cdfa9ace6613655d28fa2370fe498fbea8375f41a5fc4e643d7f8a581612fdbd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}